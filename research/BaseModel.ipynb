{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39b87ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95ea3287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\NARINDER\\\\Desktop\\\\new project\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49ad460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c86e527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\NARINDER\\\\Desktop\\\\new project'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caecba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class BaseModelConfig:\n",
    "    root_dir: Path\n",
    "    base_model_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d47ca801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "from src.floodClassifier.constants import *\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self, config_path = CONFIG_FILE_PATH, params_path = PARAMS_FILE_PATH):\n",
    "        self.config_path = Path(config_path)\n",
    "        self.params_path = Path(params_path)\n",
    "        if not self.config_path.exists():\n",
    "            raise FileNotFoundError(f\"Config file not found: {self.config_path}\")\n",
    "        if not self.params_path.exists():\n",
    "            raise FileNotFoundError(f\"Params file not found: {self.params_path}\")\n",
    "        self._config = yaml.safe_load(self.config_path.read_text())\n",
    "        self._params = yaml.safe_load(self.params_path.read_text())\n",
    "\n",
    "    def get_prepare_base_model_config(self) -> Dict[str, Any]:\n",
    "        pb = self._config.get(\"prepare_base_model\", {})\n",
    "        return {\n",
    "            \"root_dir\": pb.get(\"root_dir\", \"artifacts/prepareBaseModel\"),\n",
    "            \"base_model_path\": pb.get(\"base_model_path\", \"artifacts/prepareBaseModel/base_model.pkl\")\n",
    "        }\n",
    "\n",
    "    def get_arima_params(self) -> Dict[str, Any]:\n",
    "        model_cfg = self._params.get(\"MODEL\", {})\n",
    "        tuning_cfg = self._params.get(\"TUNING\", {})\n",
    "        forecast_cfg = self._params.get(\"FORECAST\", {})\n",
    "        metrics = self._params.get(\"METRICS\", [])\n",
    "        return {\n",
    "            \"type\": model_cfg.get(\"TYPE\", \"arima\"),\n",
    "            \"p\": int(model_cfg.get(\"P\", 1)),\n",
    "            \"d\": int(model_cfg.get(\"D\", 0)),\n",
    "            \"q\": int(model_cfg.get(\"Q\", 0)),\n",
    "            \"seasonal\": bool(model_cfg.get(\"SEASONAL\", False)),\n",
    "            \"m\": int(model_cfg.get(\"M\", 0)),\n",
    "            \"enforce_stationarity\": bool(model_cfg.get(\"ENFORCE_STATIONARITY\", True)),\n",
    "            \"enforce_invertibility\": bool(model_cfg.get(\"ENFORCE_INVERTIBILITY\", True)),\n",
    "            \"tuning_enabled\": bool(tuning_cfg.get(\"ENABLED\", False)),\n",
    "            \"tuning_search\": tuning_cfg.get(\"SEARCH\", \"grid\"),\n",
    "            \"p_values\": tuning_cfg.get(\"P_VALUES\", []),\n",
    "            \"d_values\": tuning_cfg.get(\"D_VALUES\", []),\n",
    "            \"q_values\": tuning_cfg.get(\"Q_VALUES\", []),\n",
    "            \"horizon\": int(forecast_cfg.get(\"HORIZON\", 30)),\n",
    "            \"conf_int\": float(forecast_cfg.get(\"CONF_INT\", 0.95)),\n",
    "            \"metrics\": metrics\n",
    "        }\n",
    "    \n",
    "    def get_xgboost_config(self) -> Dict[str, any]:\n",
    "            xcfg = self._config.get(\"xgboost\", {})\n",
    "            xp = self._params.get(\"XGBOOST\", {})\n",
    "            tuning_cfg = self._params.get(\"TUNING\", {}).get(\"XGBOOST\", {})\n",
    "            train_cfg = self._params.get(\"TRAINING\", {}) or {}\n",
    "\n",
    "            root_dir = Path(xcfg.get(\"root_dir\", \"artifacts/xgboost\"))\n",
    "            model_file = xcfg.get(\"model_file\", \"xgb_model.pkl\")\n",
    "            model_path = root_dir / model_file\n",
    "\n",
    "            params = {\n",
    "                \"objective\": xp.get(\"OBJECTIVE\", xp.get(\"objective\", \"binary:logistic\")),\n",
    "                \"n_estimators\": int(xp.get(\"N_ESTIMATORS\", xp.get(\"n_estimators\", 100))),\n",
    "                \"learning_rate\": float(xp.get(\"LEARNING_RATE\", xp.get(\"learning_rate\", 0.1))),\n",
    "                \"max_depth\": int(xp.get(\"MAX_DEPTH\", xp.get(\"max_depth\", 6))),\n",
    "                \"subsample\": float(xp.get(\"SUBSAMPLE\", xp.get(\"subsample\", 1.0))),\n",
    "                \"colsample_bytree\": float(xp.get(\"COLSAMPLE_BYTREE\", xp.get(\"colsample_bytree\", 1.0))),\n",
    "                \"reg_alpha\": float(xp.get(\"REG_ALPHA\", xp.get(\"reg_alpha\", 0.0))),\n",
    "                \"reg_lambda\": float(xp.get(\"REG_LAMBDA\", xp.get(\"reg_lambda\", 1.0))),\n",
    "                \"seed\": int(xp.get(\"SEED\", xp.get(\"seed\", 42))),\n",
    "                \"n_jobs\": int(xp.get(\"N_JOBS\", xp.get(\"n_jobs\", -1))),\n",
    "                \"verbosity\": int(xp.get(\"VERBOSITY\", xp.get(\"verbosity\", 1)))\n",
    "            }\n",
    "\n",
    "            train = {\n",
    "                \"test_size\": float(train_cfg.get(\"TEST_SIZE\", 0.2)),\n",
    "                \"num_boost_round\": int(train_cfg.get(\"NUM_BOOST_ROUND\", params[\"n_estimators\"])),\n",
    "                \"early_stopping_rounds\": int(train_cfg.get(\"EARLY_STOPPING_ROUNDS\", 50)),\n",
    "                \"eval_metric\": train_cfg.get(\"EVAL_METRIC\", \"logloss\"),\n",
    "                \"fit_kwargs\": train_cfg.get(\"FIT_KWARGS\", {})\n",
    "            }\n",
    "\n",
    "            tuning = {\n",
    "                \"enabled\": bool(tuning_cfg.get(\"ENABLED\", False)),\n",
    "                \"search\": tuning_cfg.get(\"SEARCH\", \"grid\"),\n",
    "                \"param_grid\": tuning_cfg.get(\"PARAM_GRID\", tuning_cfg.get(\"param_grid\", {})),\n",
    "                \"cv\": int(tuning_cfg.get(\"CV\", 3))\n",
    "            }\n",
    "\n",
    "            return {\n",
    "                \"root_dir\": str(root_dir),\n",
    "                \"model_file\": model_file,\n",
    "                \"model_path\": str(model_path),\n",
    "                \"params\": params,\n",
    "                \"train\": train,\n",
    "                \"tuning\": tuning\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4e468ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from typing import Optional, Dict\n",
    "import xgboost as xgb\n",
    "from src.floodClassifier import logger\n",
    "\n",
    "class PrepareBaseModel:\n",
    "    def __init__(self, prepare_cfg: Dict[str, str], arima_params: Dict[str, any]):\n",
    "        self.root_dir = Path(prepare_cfg[\"root_dir\"])\n",
    "        self.base_model_path = Path(prepare_cfg[\"base_model_path\"])\n",
    "        self.arima_params = arima_params\n",
    "        os.makedirs(self.root_dir, exist_ok=True)\n",
    "\n",
    "    def _ensure_series(self, y: pd.Series) -> pd.Series:\n",
    "        if not isinstance(y, pd.Series):\n",
    "            y = pd.Series(y)\n",
    "        if y.index.dtype == object:\n",
    "            try:\n",
    "                y.index = pd.to_datetime(y.index)\n",
    "            except Exception:\n",
    "                pass\n",
    "        return y.astype(float)\n",
    "\n",
    "    def build_arima(self, y: pd.Series, exog: Optional[pd.DataFrame] = None):\n",
    "        y = self._ensure_series(y)\n",
    "        p = self.arima_params[\"p\"]\n",
    "        d = self.arima_params[\"d\"]\n",
    "        q = self.arima_params[\"q\"]\n",
    "        seasonal = self.arima_params[\"seasonal\"]\n",
    "        m = self.arima_params[\"m\"] if seasonal else 0\n",
    "\n",
    "        model = SARIMAX(\n",
    "            endog=y,\n",
    "            exog=exog,\n",
    "            order=(p, d, q),\n",
    "            seasonal_order=(0, 0, 0, 0) if not seasonal else (p, d, q, m),\n",
    "            enforce_stationarity=self.arima_params.get(\"enforce_stationarity\", True),\n",
    "            enforce_invertibility=self.arima_params.get(\"enforce_invertibility\", True),\n",
    "            simple_differencing=False\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def fit_and_save(self, y: pd.Series, exog: Optional[pd.DataFrame] = None, save_overwrite: bool = True):\n",
    "        model = self.build_arima(y, exog=exog)\n",
    "        fitted = model.fit(disp=False)\n",
    "        if self.base_model_path.exists() and not save_overwrite:\n",
    "            raise FileExistsError(f\"Base model already exists at {self.base_model_path}\")\n",
    "        with open(self.base_model_path, \"wb\") as f:\n",
    "            pickle.dump(fitted, f)\n",
    "        return fitted\n",
    "\n",
    "    def load(self):\n",
    "        if not self.base_model_path.exists():\n",
    "            raise FileNotFoundError(f\"No base model at {self.base_model_path}\")\n",
    "        with open(self.base_model_path, \"rb\") as f:\n",
    "            return pickle.load(f)\n",
    "\n",
    "    def run_from_df(self, df: pd.DataFrame, target_col: str = \"y\", exog_cols: Optional[list] = None):\n",
    "        if target_col not in df.columns:\n",
    "            raise KeyError(f\"target_col '{target_col}' not found in dataframe\")\n",
    "        y = df[target_col]\n",
    "        exog = df[exog_cols] if exog_cols else None\n",
    "        fitted = self.fit_and_save(y, exog=exog)\n",
    "        return fitted\n",
    "    \n",
    "    def prepare_xgboost_model(\n",
    "        self,\n",
    "        X: pd.DataFrame,\n",
    "        y: pd.Series,\n",
    "        params_override: Optional[Dict] = None,\n",
    "        fit_kwargs: Optional[Dict] = None,\n",
    "        save_overwrite: bool = True,):\n",
    "        \"\"\"\n",
    "        Train an XGBoost model using configuration from ConfigurationManager.get_xgboost_config()\n",
    "        and save the fitted model to the configured path.\n",
    "\n",
    "        Returns the fitted model instance.\n",
    "        \"\"\"\n",
    "        # load config for xgboost\n",
    "        cfg = ConfigurationManager().get_xgboost_config()\n",
    "        model_cfg = cfg.get(\"params\", {}).copy()\n",
    "        train_cfg = cfg.get(\"train\", {}) or {}\n",
    "\n",
    "        # allow runtime overrides\n",
    "        if params_override:\n",
    "            model_cfg.update(params_override)\n",
    "\n",
    "        fit_opts = train_cfg.get(\"fit_kwargs\", {}) or {}\n",
    "        if fit_kwargs:\n",
    "            fit_opts.update(fit_kwargs)\n",
    "\n",
    "        # choose classifier vs regressor by objective string\n",
    "        objective = str(model_cfg.get(\"objective\", \"\")).lower()\n",
    "        if objective.startswith(\"reg:\") or \"reg\" in objective or \"squarederror\" in objective:\n",
    "            ModelClass = xgb.XGBRegressor\n",
    "        else:\n",
    "            ModelClass = xgb.XGBClassifier\n",
    "\n",
    "        # instantiate and fit\n",
    "        model = ModelClass(**model_cfg)\n",
    "        model.fit(X, y, **fit_opts)\n",
    "\n",
    "        # determine save location from config and persist\n",
    "        model_path = Path(cfg.get(\"model_path\", self.root_dir / \"xgb_model.pkl\"))\n",
    "        model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        if model_path.exists() and not save_overwrite:\n",
    "            raise FileExistsError(f\"XGBoost model already exists at {model_path}\")\n",
    "\n",
    "        with open(model_path, \"wb\") as f:\n",
    "            pickle.dump(model, f)\n",
    "\n",
    "        try:\n",
    "            logger.info(f\"XGBoost model trained and saved to: {model_path}\")\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9cbfdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NARINDER\\AppData\\Local\\Temp\\ipykernel_604\\1717932242.py:41: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[exog_cols] = df[exog_cols].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
      "c:\\Users\\NARINDER\\Desktop\\new project\\.venv\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\NARINDER\\Desktop\\new project\\.venv\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:473: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-21 02:55:22,635: INFO: 1717932242: ARIMA base model fitted and saved to: artifacts/prepareBaseModel/base_model.h5]\n"
     ]
    }
   ],
   "source": [
    "from src.floodClassifier import logger\n",
    "\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    prepare_cfg = config.get_prepare_base_model_config()\n",
    "    arima_params = config.get_arima_params()\n",
    "    preparer = PrepareBaseModel(prepare_cfg, arima_params)\n",
    "\n",
    "    csv_path = r\"artifacts\\data_ingestion\\FloodPrediction.csv\"\n",
    "\n",
    "    date_year_col = \"Year\"\n",
    "    date_month_col = \"Month\"\n",
    "    target_col = \"Flood?\"\n",
    "    exog_cols = [\"Altitude\", \"Max_Temp\", \"Min_Temp\", \"Relative_Humidity\"]\n",
    "    usecols = [date_year_col, date_month_col, target_col] + exog_cols\n",
    "    df = pd.read_csv(csv_path, usecols=usecols)\n",
    "\n",
    "    try:\n",
    "        df[\"Month\"] = df[date_month_col].astype(int)\n",
    "        df[\"Year\"] = df[date_year_col].astype(int)\n",
    "        df[\"_date\"] = pd.to_datetime(dict(year=df[\"Year\"], month=df[\"Month\"], day=1))\n",
    "    except Exception:\n",
    "        df[\"_date\"] = pd.to_datetime(df[date_year_col].astype(str) + \"-\" + df[date_month_col].astype(str) + \"-01\", errors=\"coerce\")\n",
    "\n",
    "    df = df.dropna(subset=[\"_date\"]).copy()\n",
    "    df = df.set_index(\"_date\")\n",
    "    df = df.sort_index()\n",
    "\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(f\"Target column '{target_col}' not found in file: {csv_path}\")\n",
    "\n",
    "    df = df.dropna(subset=[target_col])\n",
    "\n",
    "    df[target_col] = pd.to_numeric(df[target_col], errors=\"coerce\")\n",
    "    if df[target_col].isna().any():\n",
    "        raise ValueError(\"Target column contains non-numeric or uncoercible values after parsing\")\n",
    "\n",
    "    if exog_cols:\n",
    "        for col in exog_cols:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "        df[exog_cols] = df[exog_cols].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "\n",
    "    if not df.index.is_monotonic_increasing:\n",
    "        df = df.sort_index()\n",
    "\n",
    "    required_obs = max(arima_params.get(\"p\", 1), arima_params.get(\"d\", 0), arima_params.get(\"q\", 1)) + 1\n",
    "    if len(df) < required_obs:\n",
    "        raise ValueError(f\"Not enough observations ({len(df)}) to fit ARIMA with order p,d,q requiring at least {required_obs}\")\n",
    "\n",
    "    fitted_model = preparer.run_from_df(df, target_col=target_col, exog_cols=exog_cols)\n",
    "    logger.info(f\"ARIMA base model fitted and saved to: {prepare_cfg['base_model_path']}\")\n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8702fdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.42463\n",
      "[1]\tvalidation_0-logloss:0.37510\n",
      "[2]\tvalidation_0-logloss:0.33759\n",
      "[3]\tvalidation_0-logloss:0.30877\n",
      "[4]\tvalidation_0-logloss:0.28474\n",
      "[5]\tvalidation_0-logloss:0.26547\n",
      "[6]\tvalidation_0-logloss:0.24893\n",
      "[7]\tvalidation_0-logloss:0.23492\n",
      "[8]\tvalidation_0-logloss:0.22305\n",
      "[9]\tvalidation_0-logloss:0.21327\n",
      "[10]\tvalidation_0-logloss:0.20474\n",
      "[11]\tvalidation_0-logloss:0.19737\n",
      "[12]\tvalidation_0-logloss:0.19152\n",
      "[13]\tvalidation_0-logloss:0.18598\n",
      "[14]\tvalidation_0-logloss:0.18126\n",
      "[15]\tvalidation_0-logloss:0.17724\n",
      "[16]\tvalidation_0-logloss:0.17325\n",
      "[17]\tvalidation_0-logloss:0.17038\n",
      "[18]\tvalidation_0-logloss:0.16770\n",
      "[19]\tvalidation_0-logloss:0.16559\n",
      "[20]\tvalidation_0-logloss:0.16356\n",
      "[21]\tvalidation_0-logloss:0.16199\n",
      "[22]\tvalidation_0-logloss:0.16041\n",
      "[23]\tvalidation_0-logloss:0.15910\n",
      "[24]\tvalidation_0-logloss:0.15804\n",
      "[25]\tvalidation_0-logloss:0.15690\n",
      "[26]\tvalidation_0-logloss:0.15614\n",
      "[27]\tvalidation_0-logloss:0.15543\n",
      "[28]\tvalidation_0-logloss:0.15493\n",
      "[29]\tvalidation_0-logloss:0.15425\n",
      "[30]\tvalidation_0-logloss:0.15414\n",
      "[31]\tvalidation_0-logloss:0.15398\n",
      "[32]\tvalidation_0-logloss:0.15385\n",
      "[33]\tvalidation_0-logloss:0.15369\n",
      "[34]\tvalidation_0-logloss:0.15342\n",
      "[35]\tvalidation_0-logloss:0.15335\n",
      "[36]\tvalidation_0-logloss:0.15326\n",
      "[37]\tvalidation_0-logloss:0.15312\n",
      "[38]\tvalidation_0-logloss:0.15326\n",
      "[39]\tvalidation_0-logloss:0.15331\n",
      "[40]\tvalidation_0-logloss:0.15320\n",
      "[41]\tvalidation_0-logloss:0.15305\n",
      "[42]\tvalidation_0-logloss:0.15318\n",
      "[43]\tvalidation_0-logloss:0.15301\n",
      "[44]\tvalidation_0-logloss:0.15328\n",
      "[45]\tvalidation_0-logloss:0.15321\n",
      "[46]\tvalidation_0-logloss:0.15316\n",
      "[47]\tvalidation_0-logloss:0.15334\n",
      "[48]\tvalidation_0-logloss:0.15319\n",
      "[49]\tvalidation_0-logloss:0.15312\n",
      "[50]\tvalidation_0-logloss:0.15328\n",
      "[51]\tvalidation_0-logloss:0.15332\n",
      "[52]\tvalidation_0-logloss:0.15322\n",
      "[53]\tvalidation_0-logloss:0.15321\n",
      "[54]\tvalidation_0-logloss:0.15350\n",
      "[55]\tvalidation_0-logloss:0.15340\n",
      "[56]\tvalidation_0-logloss:0.15341\n",
      "[57]\tvalidation_0-logloss:0.15318\n",
      "[58]\tvalidation_0-logloss:0.15316\n",
      "[59]\tvalidation_0-logloss:0.15314\n",
      "[60]\tvalidation_0-logloss:0.15325\n",
      "[61]\tvalidation_0-logloss:0.15299\n",
      "[62]\tvalidation_0-logloss:0.15281\n",
      "[63]\tvalidation_0-logloss:0.15282\n",
      "[64]\tvalidation_0-logloss:0.15267\n",
      "[65]\tvalidation_0-logloss:0.15291\n",
      "[66]\tvalidation_0-logloss:0.15312\n",
      "[67]\tvalidation_0-logloss:0.15316\n",
      "[68]\tvalidation_0-logloss:0.15335\n",
      "[69]\tvalidation_0-logloss:0.15335\n",
      "[70]\tvalidation_0-logloss:0.15344\n",
      "[71]\tvalidation_0-logloss:0.15347\n",
      "[72]\tvalidation_0-logloss:0.15351\n",
      "[73]\tvalidation_0-logloss:0.15360\n",
      "[74]\tvalidation_0-logloss:0.15347\n",
      "[75]\tvalidation_0-logloss:0.15329\n",
      "[76]\tvalidation_0-logloss:0.15355\n",
      "[77]\tvalidation_0-logloss:0.15348\n",
      "[78]\tvalidation_0-logloss:0.15360\n",
      "[79]\tvalidation_0-logloss:0.15399\n",
      "[80]\tvalidation_0-logloss:0.15404\n",
      "[81]\tvalidation_0-logloss:0.15400\n",
      "[82]\tvalidation_0-logloss:0.15396\n",
      "[83]\tvalidation_0-logloss:0.15389\n",
      "[84]\tvalidation_0-logloss:0.15391\n",
      "[85]\tvalidation_0-logloss:0.15417\n",
      "[86]\tvalidation_0-logloss:0.15417\n",
      "[87]\tvalidation_0-logloss:0.15464\n",
      "[88]\tvalidation_0-logloss:0.15465\n",
      "[89]\tvalidation_0-logloss:0.15468\n",
      "[90]\tvalidation_0-logloss:0.15474\n",
      "[91]\tvalidation_0-logloss:0.15484\n",
      "[92]\tvalidation_0-logloss:0.15509\n",
      "[93]\tvalidation_0-logloss:0.15514\n",
      "[94]\tvalidation_0-logloss:0.15506\n",
      "[95]\tvalidation_0-logloss:0.15498\n",
      "[96]\tvalidation_0-logloss:0.15486\n",
      "[97]\tvalidation_0-logloss:0.15492\n",
      "[98]\tvalidation_0-logloss:0.15482\n",
      "[99]\tvalidation_0-logloss:0.15469\n",
      "[2025-11-21 21:12:17,825: INFO: 2247769562: XGBoost model trained and saved to artifacts\\xgboost\\xgb_model.pkl]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "csv_path = r\"artifacts\\data_ingestion\\FloodPrediction.csv\"\n",
    "target_col = \"Flood?\"\n",
    "\n",
    "try:\n",
    "    # --- Load config ---\n",
    "    config = ConfigurationManager()\n",
    "    xgb_cfg = config.get_xgboost_config()\n",
    "    params = xgb_cfg[\"params\"]\n",
    "    train_cfg = xgb_cfg[\"train\"]\n",
    "\n",
    "    model_path = Path(xgb_cfg[\"model_path\"])\n",
    "    model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- Load dataset ---\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(f\"Target column '{target_col}' not found in dataset\")\n",
    "\n",
    "    y = df[target_col].replace([np.nan, np.inf, -np.inf], 0).astype(int)\n",
    "    X = df[[\"Max_Temp\", \"Min_Temp\", \"Altitude\", \"Relative_Humidity\"]]\n",
    "\n",
    "    # --- Train/test split ---\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=train_cfg[\"test_size\"], shuffle=False\n",
    "    )\n",
    "\n",
    "    # --- Initialize and train model ---\n",
    "    model = XGBClassifier(**params)\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_val, y_val)]\n",
    "    )\n",
    "\n",
    "    # --- Save model ---\n",
    "    with open(model_path, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    logger.info(f\"XGBoost model trained and saved to {model_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.exception(\"Pipeline run failed\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a261370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NARINDER\\AppData\\Local\\Temp\\ipykernel_18512\\3938823906.py:33: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df[exog_cols] = df[exog_cols].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
      "c:\\Users\\NARINDER\\Desktop\\new project\\.venv\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:837: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n",
      "c:\\Users\\NARINDER\\Desktop\\new project\\.venv\\Lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:837: FutureWarning: No supported index is available. In the next version, calling this method in a model without a supported index will result in an exception.\n",
      "  return get_prediction_index(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG pre-align lengths -> y_test: 898 y_pred: 915\n",
      "DEBUG pre-align index ranges -> y_test: 2004-07-01 00:00:00 to 2013-12-01 00:00:00\n",
      "DEBUG y_pred index sample: DatetimeIndex(['2004-07-01', '2004-07-01', '2004-07-01'], dtype='datetime64[ns]', name='_date', freq=None) len: 915\n",
      "Evaluated samples: 898\n",
      "MAE: 0.2438\n",
      "RMSE: 0.1965\n",
      "                               SARIMAX Results                                \n",
      "==============================================================================\n",
      "Dep. Variable:                 Flood?   No. Observations:                 4493\n",
      "Model:               SARIMAX(1, 1, 1)   Log Likelihood                2168.264\n",
      "Date:                Fri, 21 Nov 2025   AIC                          -4322.528\n",
      "Time:                        07:51:20   BIC                          -4277.658\n",
      "Sample:                             0   HQIC                         -4306.716\n",
      "                               - 4493                                         \n",
      "Covariance Type:                  opg                                         \n",
      "=====================================================================================\n",
      "                        coef    std err          z      P>|z|      [0.025      0.975]\n",
      "-------------------------------------------------------------------------------------\n",
      "Rainfall              0.0001   1.35e-05      8.573      0.000    8.94e-05       0.000\n",
      "Max_Temp              0.0179      0.002      9.742      0.000       0.014       0.022\n",
      "Min_Temp              0.0445      0.002     28.545      0.000       0.041       0.048\n",
      "Relative_Humidity     0.0164      0.001     26.147      0.000       0.015       0.018\n",
      "ar.L1                -0.0256      0.017     -1.545      0.122      -0.058       0.007\n",
      "ma.L1                -0.5970      0.017    -34.142      0.000      -0.631      -0.563\n",
      "sigma2                0.0230      0.000     95.230      0.000       0.023       0.023\n",
      "===================================================================================\n",
      "Ljung-Box (L1) (Q):                  33.71   Jarque-Bera (JB):             51696.80\n",
      "Prob(Q):                              0.00   Prob(JB):                         0.00\n",
      "Heteroskedasticity (H):               1.16   Skew:                            -2.34\n",
      "Prob(H) (two-sided):                  0.00   Kurtosis:                        18.94\n",
      "===================================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from src.floodClassifier.constants import *\n",
    "\n",
    "config = ConfigurationManager()\n",
    "prepare_cfg = config.get_prepare_base_model_config()\n",
    "arima_params = config.get_arima_params()\n",
    "\n",
    "base_model_path = Path(prepare_cfg[\"base_model_path\"])\n",
    "csv_path = r\"C:\\Users\\NARINDER\\Desktop\\new project\\artifacts\\data_ingestion\\FloodPrediction.csv\"\n",
    "\n",
    "date_year_col = \"Year\"\n",
    "date_month_col = \"Month\"\n",
    "target_col = \"Flood?\"\n",
    "exog_cols = [\"Rainfall\", \"Max_Temp\"]\n",
    "test_fraction = 0.2\n",
    "forecast_horizon = arima_params.get(\"horizon\", 30)\n",
    "\n",
    "df = pd.read_csv(csv_path, usecols=[date_year_col, date_month_col, target_col] + exog_cols)\n",
    "df[\"Month\"] = df[date_month_col].astype(int)\n",
    "df[\"Year\"] = df[date_year_col].astype(int)\n",
    "df[\"_date\"] = pd.to_datetime(dict(year=df[\"Year\"], month=df[\"Month\"], day=1))\n",
    "df = df.dropna(subset=[\"_date\"])\n",
    "df = df.set_index(\"_date\").sort_index()\n",
    "df[target_col] = pd.to_numeric(df[target_col], errors=\"coerce\")\n",
    "df = df.dropna(subset=[target_col])\n",
    "if exog_cols:\n",
    "    for c in exog_cols:\n",
    "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    df[exog_cols] = df[exog_cols].fillna(method=\"ffill\").fillna(method=\"bfill\")\n",
    "\n",
    "n = len(df)\n",
    "n_test = max(1, int(n * test_fraction))\n",
    "train_df = df.iloc[:-n_test]\n",
    "test_df = df.iloc[-n_test:]\n",
    "\n",
    "y_train = train_df[target_col]\n",
    "y_test = test_df[target_col]\n",
    "exog_train = train_df[exog_cols] if exog_cols else None\n",
    "exog_test = test_df[exog_cols] if exog_cols else None\n",
    "\n",
    "if not base_model_path.exists():\n",
    "    raise FileNotFoundError(f\"Base model not found at {base_model_path}\")\n",
    "\n",
    "with open(base_model_path, \"rb\") as f:\n",
    "    fitted = pickle.load(f)\n",
    "\n",
    "steps = len(test_df)\n",
    "try:\n",
    "    fc = fitted.get_forecast(steps=steps, exog=exog_test)\n",
    "    y_pred = fc.predicted_mean\n",
    "    conf_int = fc.conf_int(alpha=1 - arima_params.get(\"conf_int\", 0.95))\n",
    "except Exception:\n",
    "    start = test_df.index[0]\n",
    "    end = test_df.index[-1]\n",
    "    y_pred = fitted.predict(start=start, end=end, exog=exog_test)\n",
    "    conf_int = None\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "try:\n",
    "    y_pred = pd.Series(y_pred)\n",
    "except Exception:\n",
    "    y_pred = pd.Series(list(y_pred))\n",
    "\n",
    "print(\"DEBUG pre-align lengths -> y_test:\", len(y_test), \"y_pred:\", len(y_pred))\n",
    "try:\n",
    "    print(\"DEBUG pre-align index ranges -> y_test:\", y_test.index.min(), \"to\", y_test.index.max())\n",
    "except Exception:\n",
    "    print(\"DEBUG y_test has no index\")\n",
    "try:\n",
    "    print(\"DEBUG y_pred index sample:\", getattr(y_pred, \"index\", None)[:3], \"len:\", len(y_pred))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    y_pred_idx = pd.Series(y_pred.values, index=test_df.index)\n",
    "except Exception:\n",
    "    y_pred_idx = pd.Series(y_pred.values[:len(test_df)], index=test_df.index[:len(y_pred.values[:len(test_df)])])\n",
    "\n",
    "y_test_s = y_test.astype(float)\n",
    "\n",
    "combined = pd.concat([y_test_s, y_pred_idx], axis=1)\n",
    "combined.columns = [\"y_true\", \"y_pred\"]\n",
    "combined = combined.dropna(how=\"any\")\n",
    "\n",
    "if combined.empty:\n",
    "    print(\"DEBUG: index alignment produced empty combined. Trying positional fallback.\")\n",
    "    min_len = min(len(y_test_s), len(y_pred))\n",
    "    if min_len > 0:\n",
    "        y_true_pos = y_test_s.values[-min_len:].astype(float)\n",
    "        y_pred_pos = y_pred.values[-min_len:].astype(float)\n",
    "        combined = pd.DataFrame({\"y_true\": y_true_pos, \"y_pred\": y_pred_pos})\n",
    "    else:\n",
    "        combined = pd.DataFrame()\n",
    "\n",
    "if combined.empty:\n",
    "    print(\"DEBUG: positional fallback produced empty. Trying element-wise non-NaN overlap.\")\n",
    "    a = y_test_s.values\n",
    "    b = y_pred.values\n",
    "    min_len = min(len(a), len(b))\n",
    "    if min_len > 0:\n",
    "        a = a[-min_len:]\n",
    "        b = b[-min_len:]\n",
    "        mask = np.isfinite(a) & np.isfinite(b)\n",
    "        if mask.any():\n",
    "            combined = pd.DataFrame({\"y_true\": a[mask], \"y_pred\": b[mask]})\n",
    "        else:\n",
    "            combined = pd.DataFrame()\n",
    "\n",
    "if combined.empty:\n",
    "    print(\"DEBUG: final diagnostics -> len(y_test):\", len(y_test), \"len(y_pred):\", len(y_pred))\n",
    "    print(\"DEBUG sample y_test tail:\", y_test.tail(5))\n",
    "    print(\"DEBUG sample y_pred tail:\", pd.Series(y_pred).tail(5))\n",
    "    raise ValueError(\"No overlapping non-NaN samples available for evaluation after all fallbacks\")\n",
    "\n",
    "y_true = combined[\"y_true\"].astype(float).values\n",
    "y_pred_vals = combined[\"y_pred\"].astype(float).values\n",
    "\n",
    "mae = mean_absolute_error(y_true, y_pred_vals)\n",
    "rmse = mean_squared_error(y_true, y_pred_vals)\n",
    "\n",
    "print(f\"Evaluated samples: {len(y_true)}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "print(fitted.summary())\n",
    "\n",
    "# future_steps = min(forecast_horizon, 12)\n",
    "# if exog_cols:\n",
    "#     last_exog = df[exog_cols].iloc[-1:]\n",
    "#     future_exog = pd.concat([last_exog]*future_steps, ignore_index=True)\n",
    "#     future_index = pd.date_range(start=df.index[-1] + pd.offsets.MonthBegin(1), periods=future_steps, freq=\"MS\")\n",
    "#     future_exog.index = future_index\n",
    "# else:\n",
    "#     future_exog = None\n",
    "#     future_index = pd.date_range(start=df.index[-1] + pd.offsets.MonthBegin(1), periods=future_steps, freq=\"MS\")\n",
    "\n",
    "# try:\n",
    "#     future_fc = fitted.get_forecast(steps=future_steps, exog=future_exog)\n",
    "#     future_pred = pd.Series(future_fc.predicted_mean.values, index=future_index)\n",
    "#     future_ci = future_fc.conf_int(alpha=1 - arima_params.get(\"conf_int\", 0.95))\n",
    "# except Exception:\n",
    "#     future_pred = pd.Series(fitted.predict(start=future_index[0], end=future_index[-1], exog=future_exog),\n",
    "#                             index=future_index)\n",
    "#     future_ci = None\n",
    "\n",
    "# print(\"\\nFuture predictions (quick):\")\n",
    "# print(future_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4cf1232d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9437819420783645\n",
      "precision 0.8\n",
      "recall 0.9586374695863747\n",
      "f1 0.872163807415606\n",
      "confusion:\n",
      " [[3090  197]\n",
      " [  34  788]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# --- Get predicted probabilities on validation set ---\n",
    "y_prob = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# --- Clip to [0,1] and threshold at 0.5 ---\n",
    "y_prob = np.clip(y_prob, 0.0, 1.0)\n",
    "y_pred_label = (y_prob >= 0.5).astype(int)\n",
    "y_true_label = y_val.astype(int)\n",
    "\n",
    "# --- Metrics ---\n",
    "print(\"accuracy\", accuracy_score(y_true_label, y_pred_label))\n",
    "print(\"precision\", precision_score(y_true_label, y_pred_label, zero_division=0))\n",
    "print(\"recall\", recall_score(y_true_label, y_pred_label, zero_division=0))\n",
    "print(\"f1\", f1_score(y_true_label, y_pred_label, zero_division=0))\n",
    "print(\"confusion:\\n\", confusion_matrix(y_true_label, y_pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ac86c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction Result ---\n",
      "Input features: {'Max_Temp': 39.0, 'Min_Temp': 29.0, 'Rainfall': 30.0, 'Relative_Humidity': 56.0, 'Wind_Speed': 44.0}\n",
      "Flood probability: 0.000\n",
      "Predicted label: No Flood\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your saved model\n",
    "MODEL_PATH = r\"artifacts\\xgboost\\xgb_model.pkl\"\n",
    "\n",
    "# Load the trained model\n",
    "with open(MODEL_PATH, \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Define the feature order (must match training)\n",
    "FEATURES = [\"Max_Temp\", \"Min_Temp\", \"Rainfall\", \"Relative_Humidity\", \"Wind_Speed\"]\n",
    "\n",
    "def get_manual_input():\n",
    "    \"\"\"\n",
    "    Collect manual input for each feature.\n",
    "    Returns a DataFrame with one row.\n",
    "    \"\"\"\n",
    "    values = []\n",
    "    for feat in FEATURES:\n",
    "        val = float(input(f\"Enter {feat}: \"))\n",
    "        values.append(val)\n",
    "    # Create a DataFrame with the same feature names\n",
    "    return pd.DataFrame([values], columns=FEATURES)\n",
    "\n",
    "def predict_flood(input_df, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Generate prediction for new input.\n",
    "    Returns probability and binary label.\n",
    "    \"\"\"\n",
    "    prob = model.predict_proba(input_df)[:, 1][0]  # probability of Flood (class 1)\n",
    "    label = int(prob >= threshold)\n",
    "    return prob, label\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Get manual input\n",
    "    new_input = get_manual_input()\n",
    "\n",
    "    # Step 2: Predict\n",
    "    prob, label = predict_flood(new_input)\n",
    "\n",
    "    # Step 3: Show results\n",
    "    print(\"\\n--- Prediction Result ---\")\n",
    "    print(f\"Input features: {new_input.to_dict(orient='records')[0]}\")\n",
    "    print(f\"Flood probability: {prob:.3f}\")\n",
    "    print(f\"Predicted label: {'Flood' if label == 1 else 'No Flood'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9a90b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
