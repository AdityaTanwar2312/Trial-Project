{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebdb0207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aee3d04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\NARINDER\\\\Desktop\\\\new project\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f4bbf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee8fd602",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\NARINDER\\\\Desktop\\\\new project'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7e5226a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class XGBoostModelConfig:\n",
    "    root_dir: Path\n",
    "    base_model_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93e68905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "from src.floodClassifier.constants import *\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self, config_path = CONFIG_FILE_PATH, params_path = PARAMS_FILE_PATH):\n",
    "        self.config_path = Path(config_path)\n",
    "        self.params_path = Path(params_path)\n",
    "        if not self.config_path.exists():\n",
    "            raise FileNotFoundError(f\"Config file not found: {self.config_path}\")\n",
    "        if not self.params_path.exists():\n",
    "            raise FileNotFoundError(f\"Params file not found: {self.params_path}\")\n",
    "        self._config = yaml.safe_load(self.config_path.read_text())\n",
    "        self._params = yaml.safe_load(self.params_path.read_text())\n",
    "\n",
    "    def get_prepare_base_model_config(self) -> Dict[str, Any]:\n",
    "        pb = self._config.get(\"prepare_base_model\", {})\n",
    "        return {\n",
    "            \"root_dir\": pb.get(\"root_dir\", \"artifacts/prepareBaseModel\"),\n",
    "            \"base_model_path\": pb.get(\"base_model_path\", \"artifacts/prepareBaseModel/base_model.pkl\")\n",
    "        }\n",
    "    def get_xgboost_config(self) -> Dict[str, any]:\n",
    "            xcfg = self._config.get(\"xgboost\", {})\n",
    "            xp = self._params.get(\"XGBOOST\", {})\n",
    "            tuning_cfg = self._params.get(\"TUNING\", {}).get(\"XGBOOST\", {})\n",
    "            train_cfg = self._params.get(\"TRAINING\", {}) or {}\n",
    "\n",
    "            root_dir = Path(xcfg.get(\"root_dir\", \"artifacts/xgboost\"))\n",
    "            model_file = xcfg.get(\"model_file\", \"xgb_model.pkl\")\n",
    "            model_path = root_dir / model_file\n",
    "\n",
    "            params = {\n",
    "                \"objective\": xp.get(\"OBJECTIVE\", xp.get(\"objective\", \"binary:logistic\")),\n",
    "                \"n_estimators\": int(xp.get(\"N_ESTIMATORS\", xp.get(\"n_estimators\", 100))),\n",
    "                \"learning_rate\": float(xp.get(\"LEARNING_RATE\", xp.get(\"learning_rate\", 0.1))),\n",
    "                \"max_depth\": int(xp.get(\"MAX_DEPTH\", xp.get(\"max_depth\", 6))),\n",
    "                \"subsample\": float(xp.get(\"SUBSAMPLE\", xp.get(\"subsample\", 1.0))),\n",
    "                \"colsample_bytree\": float(xp.get(\"COLSAMPLE_BYTREE\", xp.get(\"colsample_bytree\", 1.0))),\n",
    "                \"reg_alpha\": float(xp.get(\"REG_ALPHA\", xp.get(\"reg_alpha\", 0.0))),\n",
    "                \"reg_lambda\": float(xp.get(\"REG_LAMBDA\", xp.get(\"reg_lambda\", 1.0))),\n",
    "                \"seed\": int(xp.get(\"SEED\", xp.get(\"seed\", 42))),\n",
    "                \"n_jobs\": int(xp.get(\"N_JOBS\", xp.get(\"n_jobs\", -1))),\n",
    "                \"verbosity\": int(xp.get(\"VERBOSITY\", xp.get(\"verbosity\", 1)))\n",
    "            }\n",
    "\n",
    "            train = {\n",
    "                \"test_size\": float(train_cfg.get(\"TEST_SIZE\", 0.2)),\n",
    "                \"num_boost_round\": int(train_cfg.get(\"NUM_BOOST_ROUND\", params[\"n_estimators\"])),\n",
    "                \"early_stopping_rounds\": int(train_cfg.get(\"EARLY_STOPPING_ROUNDS\", 50)),\n",
    "                \"eval_metric\": train_cfg.get(\"EVAL_METRIC\", \"logloss\"),\n",
    "                \"fit_kwargs\": train_cfg.get(\"FIT_KWARGS\", {})\n",
    "            }\n",
    "\n",
    "            tuning = {\n",
    "                \"enabled\": bool(tuning_cfg.get(\"ENABLED\", False)),\n",
    "                \"search\": tuning_cfg.get(\"SEARCH\", \"grid\"),\n",
    "                \"param_grid\": tuning_cfg.get(\"PARAM_GRID\", tuning_cfg.get(\"param_grid\", {})),\n",
    "                \"cv\": int(tuning_cfg.get(\"CV\", 3))\n",
    "            }\n",
    "\n",
    "            return {\n",
    "                \"root_dir\": str(root_dir),\n",
    "                \"model_file\": model_file,\n",
    "                \"model_path\": str(model_path),\n",
    "                \"params\": params,\n",
    "                \"train\": train,\n",
    "                \"tuning\": tuning\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a4fe4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "from src.floodClassifier import logger\n",
    "from xgboost import XGBClassifier\n",
    "from src.floodClassifier.constants import *\n",
    "\n",
    "class PrepareBaseModel:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def train_and_save_model(self):\n",
    "        csv_path = r\"artifacts\\data_ingestion\\FloodPrediction.csv\"\n",
    "        target_col = \"Flood?\"\n",
    "\n",
    "        try:\n",
    "            # --- Load config ---\n",
    "            config = ConfigurationManager()\n",
    "            xgb_cfg = config.get_xgboost_config()\n",
    "            params = xgb_cfg[\"params\"]\n",
    "            train_cfg = xgb_cfg[\"train\"]\n",
    "\n",
    "            model_path = Path(xgb_cfg[\"model_path\"])\n",
    "            model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # --- Load dataset ---\n",
    "            df = pd.read_csv(csv_path)\n",
    "            if target_col not in df.columns:\n",
    "                raise KeyError(f\"Target column '{target_col}' not found in dataset\")\n",
    "\n",
    "            y = df[target_col].replace([np.nan, np.inf, -np.inf], 0).astype(int)\n",
    "            X = df[[\"Max_Temp\", \"Min_Temp\", \"Rainfall\", \"Relative_Humidity\", \"Wind_Speed\"]]\n",
    "\n",
    "            # --- Train/test split ---\n",
    "            from sklearn.model_selection import train_test_split\n",
    "            X_train, X_val, y_train, y_val = train_test_split(\n",
    "                X, y, test_size=train_cfg[\"test_size\"], shuffle=False\n",
    "            )\n",
    "\n",
    "            # --- Initialize and train model ---\n",
    "            model = XGBClassifier(**params)\n",
    "            model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=[(X_val, y_val)]\n",
    "            )\n",
    "\n",
    "            # --- Save model ---\n",
    "            with open(model_path, \"wb\") as f:\n",
    "                pickle.dump(model, f)\n",
    "\n",
    "            logger.info(f\"XGBoost model trained and saved to {model_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.exception(\"Pipeline run failed\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60d09da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-logloss:0.42463\n",
      "[1]\tvalidation_0-logloss:0.37510\n",
      "[2]\tvalidation_0-logloss:0.33759\n",
      "[3]\tvalidation_0-logloss:0.30877\n",
      "[4]\tvalidation_0-logloss:0.28474\n",
      "[5]\tvalidation_0-logloss:0.26547\n",
      "[6]\tvalidation_0-logloss:0.24893\n",
      "[7]\tvalidation_0-logloss:0.23492\n",
      "[8]\tvalidation_0-logloss:0.22305\n",
      "[9]\tvalidation_0-logloss:0.21327\n",
      "[10]\tvalidation_0-logloss:0.20474\n",
      "[11]\tvalidation_0-logloss:0.19737\n",
      "[12]\tvalidation_0-logloss:0.19152\n",
      "[13]\tvalidation_0-logloss:0.18598\n",
      "[14]\tvalidation_0-logloss:0.18126\n",
      "[15]\tvalidation_0-logloss:0.17724\n",
      "[16]\tvalidation_0-logloss:0.17325\n",
      "[17]\tvalidation_0-logloss:0.17038\n",
      "[18]\tvalidation_0-logloss:0.16770\n",
      "[19]\tvalidation_0-logloss:0.16559\n",
      "[20]\tvalidation_0-logloss:0.16356\n",
      "[21]\tvalidation_0-logloss:0.16199\n",
      "[22]\tvalidation_0-logloss:0.16041\n",
      "[23]\tvalidation_0-logloss:0.15910\n",
      "[24]\tvalidation_0-logloss:0.15804\n",
      "[25]\tvalidation_0-logloss:0.15690\n",
      "[26]\tvalidation_0-logloss:0.15614\n",
      "[27]\tvalidation_0-logloss:0.15543\n",
      "[28]\tvalidation_0-logloss:0.15493\n",
      "[29]\tvalidation_0-logloss:0.15425\n",
      "[30]\tvalidation_0-logloss:0.15414\n",
      "[31]\tvalidation_0-logloss:0.15398\n",
      "[32]\tvalidation_0-logloss:0.15385\n",
      "[33]\tvalidation_0-logloss:0.15369\n",
      "[34]\tvalidation_0-logloss:0.15342\n",
      "[35]\tvalidation_0-logloss:0.15335\n",
      "[36]\tvalidation_0-logloss:0.15326\n",
      "[37]\tvalidation_0-logloss:0.15312\n",
      "[38]\tvalidation_0-logloss:0.15326\n",
      "[39]\tvalidation_0-logloss:0.15331\n",
      "[40]\tvalidation_0-logloss:0.15320\n",
      "[41]\tvalidation_0-logloss:0.15305\n",
      "[42]\tvalidation_0-logloss:0.15318\n",
      "[43]\tvalidation_0-logloss:0.15301\n",
      "[44]\tvalidation_0-logloss:0.15328\n",
      "[45]\tvalidation_0-logloss:0.15321\n",
      "[46]\tvalidation_0-logloss:0.15316\n",
      "[47]\tvalidation_0-logloss:0.15334\n",
      "[48]\tvalidation_0-logloss:0.15319\n",
      "[49]\tvalidation_0-logloss:0.15312\n",
      "[50]\tvalidation_0-logloss:0.15328\n",
      "[51]\tvalidation_0-logloss:0.15332\n",
      "[52]\tvalidation_0-logloss:0.15322\n",
      "[53]\tvalidation_0-logloss:0.15321\n",
      "[54]\tvalidation_0-logloss:0.15350\n",
      "[55]\tvalidation_0-logloss:0.15340\n",
      "[56]\tvalidation_0-logloss:0.15341\n",
      "[57]\tvalidation_0-logloss:0.15318\n",
      "[58]\tvalidation_0-logloss:0.15316\n",
      "[59]\tvalidation_0-logloss:0.15314\n",
      "[60]\tvalidation_0-logloss:0.15325\n",
      "[61]\tvalidation_0-logloss:0.15299\n",
      "[62]\tvalidation_0-logloss:0.15281\n",
      "[63]\tvalidation_0-logloss:0.15282\n",
      "[64]\tvalidation_0-logloss:0.15267\n",
      "[65]\tvalidation_0-logloss:0.15291\n",
      "[66]\tvalidation_0-logloss:0.15312\n",
      "[67]\tvalidation_0-logloss:0.15316\n",
      "[68]\tvalidation_0-logloss:0.15335\n",
      "[69]\tvalidation_0-logloss:0.15335\n",
      "[70]\tvalidation_0-logloss:0.15344\n",
      "[71]\tvalidation_0-logloss:0.15347\n",
      "[72]\tvalidation_0-logloss:0.15351\n",
      "[73]\tvalidation_0-logloss:0.15360\n",
      "[74]\tvalidation_0-logloss:0.15347\n",
      "[75]\tvalidation_0-logloss:0.15329\n",
      "[76]\tvalidation_0-logloss:0.15355\n",
      "[77]\tvalidation_0-logloss:0.15348\n",
      "[78]\tvalidation_0-logloss:0.15360\n",
      "[79]\tvalidation_0-logloss:0.15399\n",
      "[80]\tvalidation_0-logloss:0.15404\n",
      "[81]\tvalidation_0-logloss:0.15400\n",
      "[82]\tvalidation_0-logloss:0.15396\n",
      "[83]\tvalidation_0-logloss:0.15389\n",
      "[84]\tvalidation_0-logloss:0.15391\n",
      "[85]\tvalidation_0-logloss:0.15417\n",
      "[86]\tvalidation_0-logloss:0.15417\n",
      "[87]\tvalidation_0-logloss:0.15464\n",
      "[88]\tvalidation_0-logloss:0.15465\n",
      "[89]\tvalidation_0-logloss:0.15468\n",
      "[90]\tvalidation_0-logloss:0.15474\n",
      "[91]\tvalidation_0-logloss:0.15484\n",
      "[92]\tvalidation_0-logloss:0.15509\n",
      "[93]\tvalidation_0-logloss:0.15514\n",
      "[94]\tvalidation_0-logloss:0.15506\n",
      "[95]\tvalidation_0-logloss:0.15498\n",
      "[96]\tvalidation_0-logloss:0.15486\n",
      "[97]\tvalidation_0-logloss:0.15492\n",
      "[98]\tvalidation_0-logloss:0.15482\n",
      "[99]\tvalidation_0-logloss:0.15469\n",
      "[2025-11-21 22:20:52,748: INFO: 1471796507: XGBoost model trained and saved to artifacts\\xgboost\\xgb_model.pkl]\n"
     ]
    }
   ],
   "source": [
    "prepare_base_model = PrepareBaseModel()\n",
    "prepare_base_model.train_and_save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ebc0930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9437819420783645\n",
      "precision 0.8\n",
      "recall 0.9586374695863747\n",
      "f1 0.872163807415606\n",
      "confusion:\n",
      " [[3090  197]\n",
      " [  34  788]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "# Path to your saved model\n",
    "MODEL_PATH = r\"artifacts\\xgboost\\xgb_model.pkl\"\n",
    "\n",
    "# Load the trained model\n",
    "with open(MODEL_PATH, \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "csv_path = r\"artifacts\\data_ingestion\\FloodPrediction.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "target_col = \"Flood?\"\n",
    "\n",
    "y = df[target_col].replace([np.nan, np.inf, -np.inf], 0).astype(int)\n",
    "X = df[[\"Max_Temp\", \"Min_Temp\", \"Rainfall\", \"Relative_Humidity\", \"Wind_Speed\"]]\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False\n",
    ")\n",
    "\n",
    "# --- Get predicted probabilities on validation set ---\n",
    "y_prob = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# --- Clip to [0,1] and threshold at 0.5 ---\n",
    "y_prob = np.clip(y_prob, 0.0, 1.0)\n",
    "y_pred_label = (y_prob >= 0.5).astype(int)\n",
    "y_true_label = y_val.astype(int)\n",
    "\n",
    "# --- Metrics ---\n",
    "print(\"accuracy\", accuracy_score(y_true_label, y_pred_label))\n",
    "print(\"precision\", precision_score(y_true_label, y_pred_label, zero_division=0))\n",
    "print(\"recall\", recall_score(y_true_label, y_pred_label, zero_division=0))\n",
    "print(\"f1\", f1_score(y_true_label, y_pred_label, zero_division=0))\n",
    "print(\"confusion:\\n\", confusion_matrix(y_true_label, y_pred_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14959147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'objective': 'binary:logistic',\n",
       " 'base_score': None,\n",
       " 'booster': None,\n",
       " 'callbacks': None,\n",
       " 'colsample_bylevel': None,\n",
       " 'colsample_bynode': None,\n",
       " 'colsample_bytree': 1.0,\n",
       " 'device': None,\n",
       " 'early_stopping_rounds': None,\n",
       " 'enable_categorical': False,\n",
       " 'eval_metric': None,\n",
       " 'feature_types': None,\n",
       " 'feature_weights': None,\n",
       " 'gamma': None,\n",
       " 'grow_policy': None,\n",
       " 'importance_type': None,\n",
       " 'interaction_constraints': None,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_bin': None,\n",
       " 'max_cat_threshold': None,\n",
       " 'max_cat_to_onehot': None,\n",
       " 'max_delta_step': None,\n",
       " 'max_depth': 6,\n",
       " 'max_leaves': None,\n",
       " 'min_child_weight': None,\n",
       " 'missing': nan,\n",
       " 'monotone_constraints': None,\n",
       " 'multi_strategy': None,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': -1,\n",
       " 'num_parallel_tree': None,\n",
       " 'random_state': None,\n",
       " 'reg_alpha': 0.0,\n",
       " 'reg_lambda': 1.0,\n",
       " 'sampling_method': None,\n",
       " 'scale_pos_weight': None,\n",
       " 'subsample': 1.0,\n",
       " 'tree_method': None,\n",
       " 'validate_parameters': None,\n",
       " 'verbosity': 1,\n",
       " 'seed': 42}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained model\n",
    "with open(MODEL_PATH, \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b66aee7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction Result ---\n",
      "Input features: {'Max_Temp': 33.0, 'Min_Temp': 26.0, 'Rainfall': 575.0, 'Relative_Humidity': 85.0, 'Wind_Speed': 1.57}\n",
      "Flood probability: 0.888\n",
      "Predicted label: Flood\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your saved model\n",
    "MODEL_PATH = r\"artifacts\\xgboost\\xgb_model.pkl\"\n",
    "\n",
    "# Load the trained model\n",
    "with open(MODEL_PATH, \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "# Define the feature order (must match training)\n",
    "FEATURES = [\"Max_Temp\", \"Min_Temp\", \"Rainfall\", \"Relative_Humidity\", \"Wind_Speed\"]\n",
    "\n",
    "def get_manual_input():\n",
    "    \"\"\"\n",
    "    Collect manual input for each feature.\n",
    "    Returns a DataFrame with one row.\n",
    "    \"\"\"\n",
    "    values = []\n",
    "    for feat in FEATURES:\n",
    "        val = float(input(f\"Enter {feat}: \"))\n",
    "        values.append(val)\n",
    "    # Create a DataFrame with the same feature names\n",
    "    return pd.DataFrame([values], columns=FEATURES)\n",
    "\n",
    "def predict_flood(input_df, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Generate prediction for new input.\n",
    "    Returns probability and binary label.\n",
    "    \"\"\"\n",
    "    prob = model.predict_proba(input_df)[:, 1][0]  # probability of Flood (class 1)\n",
    "    label = int(prob >= threshold)\n",
    "    return prob, label\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Get manual input\n",
    "    new_input = get_manual_input()\n",
    "\n",
    "    # Step 2: Predict\n",
    "    prob, label = predict_flood(new_input)\n",
    "\n",
    "    # Step 3: Show results\n",
    "    print(\"\\n--- Prediction Result ---\")\n",
    "    print(f\"Input features: {new_input.to_dict(orient='records')[0]}\")\n",
    "    print(f\"Flood probability: {prob:.3f}\")\n",
    "    print(f\"Predicted label: {'Flood' if label == 1 else 'No Flood'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813ec385",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
